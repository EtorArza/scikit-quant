{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5641925",
   "metadata": {},
   "source": [
    "Hubbard Model Workflow Example\n",
    "============================\n",
    "\n",
    "Prerequisites: OpenFermion, Qiskit, BQSKit, scikit-quant, pyDOE, and SciPy, e.g. install with pip:\n",
    "\n",
    "`$ python -m pip install qiskit qiskit-aer qiskit_algorithms openfermion bqskit scikit-quant pyDOE scikit-learn scipy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01231b7b",
   "metadata": {},
   "source": [
    "Introduction\n",
    "--------------\n",
    "\n",
    "This notebook exercises a couple of different workflow choices to show how alternatives setups and methods fit together. It is by necessity not exhaustive and since it runs on a small example (in the interest of time) that easily and quickly runs to completion regardless, the benefits of different choices will not be obvious in the results. (Differences in outcome could also simply be due to noise, thus any true comparison of techniques should  be over an average of several runs.) However, the focus here is on underlying reasons for those choices, the implementation mechanics, and what to look out for, such that the techniques can be applied to different optimization problems where there would be a benefit.\n",
    "\n",
    "In a real experiment, there would typically also be a post-processing step to reduce the number of misclassifications.$^1$ This can, however, be simulated by reducing the measurement error (`p_meas`, below). For larger experiments, since the Variational Quantum Eigensolver (VQE) algorithm scales rather badly, it is also worthwhile to look into techniques to reduce the total number of component measurements required.$^2$\n",
    "\n",
    "First a recap of some of the available choices that we covered in the notebook so far.\n",
    "\n",
    "**1. Is a good initial available?** If yes, go to 2. If not, then some sort of global search needs to happen (which can be very costly). Most global searches consist of some form of random sampling and need guidance or at least boundaries delimiting the search space. If local minima are expected, a multistart$^3$ method should be used: starting a \"global\" search in several, judiciously chosen, locations on the optimization surface. The global search need not converge or find the global minimum: it just needs to get close enough to provide a good initial. Having determined this initial, good, or at least tighter, bounds can be set on a region around it.\n",
    "<br>**2. Are good bounds available?** Bounds are useful to reduce the overall search space; some optimizers (e.g. if stencil-based) even need them or they won't run or will scale so badly to be unusable. Other optimizers, e.g. trust region methods, are naturally limited in their search extent and may never reach the bounds if the initial is good and a large distance away. Bounds can even be harmful, e.g. a gradient-based method may benefit from following the gradient across a boundary, assuming parameters outside the bounds are still computationally (if not physically) valid: temporarily crossing the boundary may bring them back into the search space to a global minimum that is not otherwise reachable when starting descend from the initial. In short, the availability of good bounds is an input into the decision of which optimizer to choose, and conversely, if an optimizer is already settled upon, the bounds may need to be updated. Go to 3.\n",
    "<br>**3. What does the noise look like?** Noise deforms the optimization surface (this is a separate question from the one of __[how noise affects VQE](hubbard_vqe_noise.ipynb)__), creating local minima and setting up fake gradients. The magnitude and frequency, and whether noise is correlated or stochastic, all drive the decision of which optimizer to use, but also what convergence criteria to pick. If nothing is known a priori, then two generalities can still be observed:\n",
    "   * Initially, further away from the minimum, gradients are steeper and the impact of noise will be less, even if large and of high frequency.\n",
    "   * Closer to the minimum, the optimization surface flattens, and the impact of noise will be worse, even if small and of low frequency.\n",
    "\n",
    "Closer to the minimum, smoothing out the noise becomes more important. Either do this explicitly in the evaluation (i.e. independent of the optimizer), implicitly by e.g. using Gaussian Process models, or by choosing an optimizer that uses local models.\n",
    "<br>**4. What other resources are available?** If sufficient CPU time is available during the classical step, a tool such as __[BQSKit](https://bqskit.lbl.gov/)__ can be used to produce optimized (shortened) circuits. BQSKit has different methods and settings for all CPU/time budgets and running a single iteration may already be worth it.\n",
    "\n",
    "**Exercise:** in the workflow below, at each step, consider whether you agree with the setup choices (chosen method, number of Trotter steps, use of BQSKit, max number of evaluations, etc.). Modify as you see fit.\n",
    "\n",
    "Let's get started. First, create once more a __[Hubbard Model](hubbard_model_intro.ipynb)__ setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hubbard as hb\n",
    "import logging\n",
    "import noise_model as noise\n",
    "import numpy as np\n",
    "import opti_by_gp as obg\n",
    "\n",
    "logging.getLogger('hubbard').setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca102e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a model appropriate for the machine used:\n",
    "#    laptop -> use small model\n",
    "#    server -> use medium model\n",
    "\n",
    "MODEL = hb.small_model\n",
    "#MODEL = hb.medium_model\n",
    "\n",
    "# Hubbard model for fermions (Fermi-Hubbard) required parameters\n",
    "xdim, ydim, t, U, chem, magf, periodic, spinless = MODEL()\n",
    "\n",
    "# Number of electrons to add to the system\n",
    "n_electrons_up   = 1\n",
    "n_electrons_down = 1\n",
    "n_electrons = n_electrons_up + n_electrons_down\n",
    "\n",
    "# Total number of \"sites\", with each qubit representing occupied or not\n",
    "spinfactor = spinless and 1 or 2\n",
    "n_qubits = n_sites = xdim * ydim * spinfactor\n",
    "\n",
    "# Create the Hubbard Model for use with Qiskit\n",
    "hubbard_op = hb.hamiltonian_qiskit(\n",
    "    x_dimension        = xdim,\n",
    "    y_dimension        = ydim,\n",
    "    tunneling          = t,\n",
    "    coulomb            = U,\n",
    "    chemical_potential = chem,\n",
    "    magnetic_field     = magf,\n",
    "    periodic           = periodic,\n",
    "    spinless           = spinless)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1e685a",
   "metadata": {},
   "source": [
    "To verify our results, it's useful to know the answer (this is often not possible in practice, of course). There are two ways to determine success: whether the ground energy estimate found is close to the exact solution, or whether the optimal parameters found provide an estimate close to the exact solution. Here, we opt for the latter, as it is a more consistent measure. Thus, we need the exact solution and an \"exact\" object function to use for evaluation of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e0d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For record keeping, get the exact energy level\n",
    "exact = hb.exact(hubbard_op, n_electrons_up, n_electrons_down)\n",
    "\n",
    "# For verification of results, construct a noise-free, Trotter-free objective\n",
    "objective_exact = hb.EnergyObjective(hubbard_op, n_electrons_up, n_electrons_down,\n",
    "    trotter_steps=-1, noise_model=None, shots=-1, run_bqskit=False)\n",
    "\n",
    "def res_energy(result):\n",
    "    try:\n",
    "        return result.fun\n",
    "    except AttributeError:\n",
    "        return result[1]\n",
    "\n",
    "def res_nevals(result):\n",
    "    try:\n",
    "        return result.nfev\n",
    "    except AttributeError:\n",
    "        return result[2]\n",
    "\n",
    "def res_x(result):\n",
    "    try:\n",
    "        return result.x\n",
    "    except AttributeError:\n",
    "        return result[0]\n",
    "    \n",
    "def verify(result):\n",
    "    print('Energy estimate:', res_energy(result))\n",
    "    print('Number of evaluations:', res_nevals(result))\n",
    "    print('Noise-free energy at result:', objective_exact(res_x(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f095631",
   "metadata": {},
   "source": [
    "Further ingredients are a couple of optimizers to play with. The ones below are all available through Qiskit, but optimizer interfaces are similar enough to adjust the code below to use your favorite optimizer (e.g. from __[`scipy.optimize`](https://docs.scipy.org/doc/scipy/reference/optimize.html)__).\n",
    "\n",
    "Create a noise model, the exact (device-specific) details of which will drive most decision making in practice (see the notebook on the __[effects of noise on VQE](hubbard_vqe_noise.ipynb)__). The misclassification (`p_meas`) can realistically be set much lower than typical raw error rates, if you employ effective post-processing techniques.$^1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in a couple of optimizers to play with\n",
    "from qiskit_algorithms.optimizers import COBYLA, SPSA\n",
    "try:\n",
    "    from qiskit_algorithms.optimizers import IMFIL, SNOBFIT\n",
    "except ImportError:\n",
    "    print(\"install scikit-quant to use IMFIL and SNOBFIT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfefb794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a noise model; change parameter (or even the whole model: this\n",
    "# only considers misclassification and depolarization errors) as desired\n",
    "p_meas = 0.01       # this can be set much lower to \"simulate\" post-processing\n",
    "p_gate_1q = 5.E-5\n",
    "p_gate_2q = 3.E-4\n",
    "\n",
    "NOISE_MODEL = noise.create(p_gate_1q=p_gate_1q, p_gate_2q=p_gate_2q, p_meas=p_meas)\n",
    "print(NOISE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa076935",
   "metadata": {},
   "source": [
    "**Optional Exercise:** we have seen that a minimum of 3 Trotter steps are needed and the default number of shots on IBM devices is 8192 samples. The code below constructs an objective to match and then selects the Qiskit-recommended SPSA optimizer to find the minimum. Note that SPSA will evaluate the objective _at least_ twice per iteration, thus setting `maxiter` to $N$ may actually lead to a multiple of $N$ more evaluations. The point of the exercise is to give a rough idea about the level of difficulty of the chosen models if all defaults are used and no mitigating steps are taken. In particular, the small model should resolve just fine, assuming the number of iterations allowed is commensurate with the noise level, with SPSA only every once in a while getting stuck in a noise-induced local minimum. Experiment with increased shot count and running BQSKit to reduce noise, and how much the problem simplifies with a good initial (set `good=True` in the `MODEL.initial` call below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fcfa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code is commented out, because it is an optional exercise; this way it does\n",
    "# not accidentally run if the whole notebook is executed\n",
    "\"\"\"\\\n",
    "objective = hb.EnergyObjective(hubbard_op, n_electrons_up, n_electrons_down,\n",
    "    trotter_steps=3, noise_model=NOISE_MODEL, shots=8192, run_bqskit=False)\n",
    "\n",
    "initial_amplitudes, bounds = MODEL.initial(\n",
    "    n_electrons_up, n_electrons_down, objective.npar(), good=False)\n",
    "\n",
    "result = SPSA(maxiter=50).minimize(     # will actually do 100 evaluations\n",
    "    objective, initial_amplitudes, bounds=bounds)\n",
    "\n",
    "verify(result)\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99917610",
   "metadata": {},
   "source": [
    "The first step will have to be a global search and for that reason, ImFil is chosen. The number of Trotter steps is lowered to reduce noise, shots at default, BQSKit can be run if enough classical compute is available, but here it is switched off as far away from the global minimum, the impact of noise is less and ImFil is equipped to handle high noise (alternative `run_bqskit` settings are `True` to run 1 cycle, which will capture most improvement; or `'full'` to run until convergence, if classical resources are not limiting).\n",
    "\n",
    "With high noise, ImFil is unlikely to use up its full budget: once it gets close to a minimum, the optimization surface flattens and it will not find any gradients on its stencils, which subsequently \"fail.\" This is a useful property, as it leaves more of the budget to the next step, rather than wasting it on chasing fake, noise-induced, gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4c7344",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_evals = 0\n",
    "\n",
    "objective = hb.EnergyObjective(hubbard_op, n_electrons_up, n_electrons_down,\n",
    "    trotter_steps=2, noise_model=NOISE_MODEL, shots=8192, run_bqskit=False)\n",
    "\n",
    "initial_amplitudes, bounds = MODEL.initial(\n",
    "    n_electrons_up, n_electrons_down, objective.npar(), good=False)\n",
    "\n",
    "result = IMFIL(maxiter=50).optimize(objective, initial_amplitudes[:], bounds=bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c771ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "verify(result)\n",
    "\n",
    "# Keep track of the total number of evaluations used\n",
    "total_evals += res_nevals(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af6028e",
   "metadata": {},
   "source": [
    "After the global search, we can now expect (hope) to be close to the global minimum, which means that the surface flattens and some manner of noise smoothing is required. The number of Trotter steps needs to be increased, as relative differences have more impact close to the minimum. Each extra Trotter step causes an increase in noise (because of an increase in circuit depth), thus, if resources are available, enable BQSkit. (In the interest of time, we run only a single iteration, by setting `run_bqskit=True`.)\n",
    "\n",
    "Bounds can be tightened. This should be guided by the first of the last failed stencils from ImFil, but the Qiskit optimizer interface makes access to such search details currently not possible. Here, we stick to a guesstimated, but relatively loose, bound. In any case, since ImFil (used under the hood by the Gaussian Process models implementation) always searches the boundaries, if the bounds are too tight, this will show up in the optimal parameters found, as these would be on the boundary. If that happens, go back and loosen the bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40247728",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = hb.EnergyObjective(hubbard_op, n_electrons_up, n_electrons_down,\n",
    "    trotter_steps=3, noise_model=NOISE_MODEL, shots=8192, run_bqskit=True, save_evals=True)\n",
    "\n",
    "initial_amplitudes = res_x(result)[:]\n",
    "bounds = np.zeros((objective.npar(), 2))\n",
    "bounds[:,0] = np.subtract(initial_amplitudes, 0.1)\n",
    "bounds[:,1] = np.add(     initial_amplitudes, 0.1)\n",
    "\n",
    "maxevals = 20\n",
    "\n",
    "result = obg.opti_by_gp(objective.npar(), bounds, objective, maxevals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f39483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "verify(result)\n",
    "\n",
    "# Keep track of the total number of evaluations used\n",
    "total_evals += res_nevals(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a84221",
   "metadata": {},
   "source": [
    "Finally, run SnobFit as a last refinement, as its model fits should smooth out the noise effectively and take advantage of the local symmetry (this is not generally applicable, but often the case for physical systems when close to the global minimum, as most things in nature are continuous). To further reduce noise, consider increasing the number of shots (not done here, to keep comparison simpler) and run BQSKit until convergence. Tighten the bounds once more, but be on the lookout for a result at or near the boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbc2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = hb.EnergyObjective(hubbard_op, n_electrons_up, n_electrons_down,\n",
    "    trotter_steps=3, noise_model=NOISE_MODEL, shots=8192, run_bqskit=\"full\")\n",
    "\n",
    "initial_amplitudes = result[0][:]\n",
    "bounds = np.zeros((objective.npar(), 2))\n",
    "bounds[:,0] = np.subtract(initial_amplitudes, 0.05)\n",
    "bounds[:,1] = np.add(     initial_amplitudes, 0.05)\n",
    "\n",
    "result = SNOBFIT(maxiter=30).optimize(objective, initial_amplitudes[:], bounds=bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155acb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "verify(result)\n",
    "\n",
    "total_evals += res_nevals(result)\n",
    "print('Total number of evaluations used:', total_evals)\n",
    "# Keep track of the total number of evaluations used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e1944",
   "metadata": {},
   "source": [
    "References\n",
    "-------------\n",
    "\n",
    "[1] https://arxiv.org/abs/1910.01969, B. Nachman, et al., \"Unfolding Quantum Computer Readout Noise\"\n",
    "<br>[2] https://arxiv.org/abs/1907.13623, P. Gokhale, et al., \"Minimizing State Preparations in Variational Quantum Eigensolver by Partitioning into Commuting Families\"\n",
    "<br>[3] https://arxiv.org/abs/1905.08768, R. Shaydulin, et al., \"Multistart Methods for Quantum Approximate Optimization\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
