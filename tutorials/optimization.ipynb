{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d61d3bfa",
   "metadata": {},
   "source": [
    "# Introduction to Gaussian Process Models\n",
    "Gaussian process (GP) models serve as approximations of computationally expensive (time-consuming) black-box functions. To reduce the number of times the expensive function must be queried during optimization, the GP is used to guide the sampling decisions in the parameter space and only the \"most promising\" parameters are selected for evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a449c",
   "metadata": {},
   "source": [
    "A GP model treats the function it approximates like the realization of a stochastic process: \n",
    "$m_{GP}(\\theta) = \\mu + Z(\\theta)$,\n",
    "where $\\mu$ represents the mean of the stochastic process and $Z(\\theta) \\sim \\mathcal{N}(0,\\sigma^2)$ is the deviation from the mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e571106",
   "metadata": {},
   "source": [
    "The correlation between two random variables $Z(\\theta_k)$ and $Z(\\theta_l)$ is defined by a kernel, e.g., the squared exponential (also Radial basis function) kernel: \n",
    "\\begin{equation}\n",
    "Corr(Z(\\theta_k),Z(\\theta_l)) = \\exp(-\\sum_{i=1}^d \\gamma_i|\\theta_k^{(i)}-\\theta_l^{(i)}|^{q_i})\n",
    "\\end{equation}, \n",
    "with $\\gamma_i$ determining how quickly the correlation in dimension $i$ decreases, and $q_i$ refelcts the smoothness of the function in dimension $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0545147c",
   "metadata": {},
   "source": [
    "Denoting $\\mathbf{R}$ as the matrix whose $(k,l)$-th element is given as the correlation above, maximum likelihood estimation is used to determine the GP parameters $\\mu$, $\\sigma^2$, and $\\gamma_i$. Then, at an unsampled point $\\theta^{new}$, the GP prediction is \\begin{equation}\n",
    "    m_{\\text{GP}}(\\theta^{\\text{new}})=\\hat{\\mu}+\\mathbf{r}^T\\mathbf{R}^{-1}(\\mathbf{f}-\\mathbf{1}\\hat\\mu),\n",
    "\\end{equation}\n",
    "where $\\mathbf{1}$ is a vector of ones of appropriate dimension and $\\mathbf{f}$ is the vector of function values obtained so far, and\n",
    "\\begin{equation}\n",
    "    \\boldsymbol{r}=\n",
    "    \\begin{bmatrix}\n",
    "    Corr\\left(Z(\\theta^{\\text{new}}), Z(\\theta_1)\\right)\\\\\n",
    "    \\vdots\\\\\n",
    "    Corr\\left(Z(\\theta^{\\text{new}}\n",
    "    ), Z(\\theta_n)\\right)\n",
    "    \\end{bmatrix}.\n",
    "\\end{equation}\n",
    "The  corresponding  mean squared error is \n",
    "\\begin{equation}\n",
    "    s^2(\\theta^{\\text{new}})=\\hat{\\sigma}^2\\left(   1-\\boldsymbol{r}^T\\boldsymbol{R}^{-1}\\boldsymbol{r} +\\frac{(1-\\boldsymbol{1}^T\\boldsymbol{R}^{-1}\\boldsymbol{r})^2}{\\mathbf{1}^T\\boldsymbol{R}^{-1}\\mathbf{1}}\\right)\n",
    "\\end{equation}\n",
    "with \n",
    " \\begin{equation}\n",
    "     \\hat{\\mu} = \\frac{\\mathbf{1}^T\\boldsymbol{R}^{-1}\\mathbf{f}}{\\mathbf{1}^T\\boldsymbol{R}^{-1}\\mathbf{1}}\n",
    " \\end{equation}\n",
    " and\n",
    " \\begin{equation}\n",
    "     \\hat{\\sigma}^2=\\frac{(\\mathbf{f}-\\mathbf{1}\\hat{\\mu})^T\\boldsymbol{R}^{-1}(\\mathbf{f}-\\mathbf{1}\\hat{\\mu})}{n}.\n",
    " \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e08111",
   "metadata": {},
   "source": [
    "Python has a good implementation of GPs where you can choose different kernels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06bf024",
   "metadata": {},
   "source": [
    "First, we need (input, output) data pairs. Inputs are parameters where we query the function (for simplicity, the example has an inexpensive function). From the Sckit-Learn website: https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc90a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, Matern, RationalQuadratic, ExpSineSquared, WhiteKernel\n",
    "from scipy.optimize import minimize\n",
    "from scipy.spatial import distance\n",
    "import scipy.spatial as scp\n",
    "from scipy.stats import norm\n",
    "from pyDOE import * #needed if Latin hypercube design is used\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"The function we want to approximate.\"\"\"\n",
    "    return x * np.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26edbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlow = 0 #lower bound on x\n",
    "xup = 10 #upper bound on x\n",
    "dim = 1 #dimension of the problem\n",
    "lhs_wanted = False\n",
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ee84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(lhs_wanted): #when not using space-filling design\n",
    "    X = np.atleast_2d([1., 3., 7., 8.]).T #select some points where we evaluate the function\n",
    "\n",
    "    # Function evaluations\n",
    "    y = f(X).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1870ff7",
   "metadata": {},
   "source": [
    "Other options for creating space filling designs is latin hypercube sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4beb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhs_wanted:\n",
    "    ninit=6 #6 initial evaluations\n",
    "    init_design = lhs(dim, samples =ninit, criterion='maximin') #initial design in [0,1]^dim\n",
    "    X = xlow+(xup-xlow)*init_design #scale to [xlow,xup]\n",
    "    # Function evaluations\n",
    "    y = f(X).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1d660c",
   "metadata": {},
   "source": [
    "**Exercise:** run the code with different initial samples, i.e., try lhs_wanted = False and lhs_wanted = True and compare the sampling history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e495aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a GP kernel (here RBF or squared exponential)\n",
    "kernel = RBF()\n",
    "gp = GaussianProcessRegressor(kernel=kernel, normalize_y=True,n_restarts_optimizer=9)\n",
    "\n",
    "# Fit the GP to the input-output data \n",
    "gp.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03311c38",
   "metadata": {},
   "source": [
    "Make some good-looking plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f178ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_the_gp(X, y, gp, xnew):\n",
    "    #select a bunch of points where we want to make predictions wioth the GP\n",
    "    x = np.atleast_2d(np.linspace(0, 10, 1000)).T \n",
    "    # Make the GP prediction at the points where no evaluations were taken - also return predicted uncertainty\n",
    "    y_pred, sigma = gp.predict(x, return_std=True)\n",
    "    plt.figure()\n",
    "    plt.plot(x, f(x), 'r:', label=r'$f(x) = x\\,\\sin(x)$')\n",
    "    plt.plot(X, y, 'r.', markersize=10, label='Observations')\n",
    "    plt.plot(x, y_pred, 'b-', label='Prediction')\n",
    "    if len(xnew)>0:\n",
    "        plt.plot(X[-1], y[-1], 'gs', markersize=10, label='Newest sample')\n",
    "    plt.fill(np.concatenate([x, x[::-1]]),\n",
    "         np.concatenate([y_pred - 1.9600 * sigma,\n",
    "                        (y_pred + 1.9600 * sigma)[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$f(x)$')\n",
    "    plt.ylim(-10, 20)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a13256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_the_gp(X, y, gp, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ff8b75",
   "metadata": {},
   "source": [
    "**Optional Exercise:** check out the Scikit-Learn website https://scikit-learn.org/stable/modules/gaussian_process.html#kernels-for-gaussian-processes and experiment around with different basic kernels, kernel parameters and kernel combinations, e.g., \n",
    "- does using \"kernel = RBF(10, (1e-2, 1e2))\" change anything?\n",
    "- what happens when you use \"kernel = Matern(length_scale=1.0, nu=1.5)\"\n",
    "- try \"kernel = 1.0 * RationalQuadratic(length_scale=1.0, alpha=0.1, alpha_bounds=(1e-5, 1e15))\"\n",
    "- \"kernel = 1.0 * ExpSineSquared(\n",
    "    length_scale=1.0,\n",
    "    periodicity=3.0,\n",
    "    length_scale_bounds=(0.1, 10.0),\n",
    "    periodicity_bounds=(1.0, 10.0),)\"\n",
    "- use a combination of kernels: \"kernel = RBF()+WhiteKernel(noise_level=.001)\" using different noise_levels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1ed918",
   "metadata": {},
   "source": [
    "**Exercise:** Change the inputs of the GP (i.e., the training samples) and see how the GP predictions change (use fewer or more points, use different points in [0,10], e.g., \"X=np.atleast_2d(np.random.uniform(0,10,5)).T\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3813da9",
   "metadata": {},
   "source": [
    "Takeaway: the quality and accuracy of the GP highly depends on the trianing data and the kernel used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1959e3",
   "metadata": {},
   "source": [
    "# Adaptive Optimization with the GP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7a3b2d",
   "metadata": {},
   "source": [
    "GP models are often used in optimization algorithms. In each iteration of the optimization, a new sample point is selected by maximizing the expected improvement (EI):\n",
    "\\begin{equation}\n",
    "    \\mathbb{E}(I)(\\theta) = s(\\theta)\\left(v\\Phi(v)+\\phi(v)   \\right),\n",
    "\\end{equation}\n",
    "where \n",
    "\\begin{equation}\n",
    "    v=\\frac{f^{\\text{best}}-m_{\\text{GP}}(\\theta)}{s(\\theta)}\n",
    "\\end{equation}\n",
    "and $\\Phi$ and $\\phi$ are the normal cumulative distribution and density functions, respectively, and $s(\\theta)=\\sqrt{s^2(\\theta)}$ is the square root of the mean squared error. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790176e5",
   "metadata": {},
   "source": [
    "The function $\\mathbb{E}(I)(\\theta)$ can be maximized with any python optimization library. The point $\\theta^{\\text{new}}$ where it reaches its maximum will be the new point where $f$ is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61a4542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define expected improvement function\n",
    "def ei(x, gpr_obj, Xsamples, Ysamples): #expected improvement\n",
    "    dim = len(x)\n",
    "    x= x.reshape(1, -1)\n",
    "\n",
    "    min_dist=np.min(scp.distance.cdist(x, Xsamples))\n",
    "    if min_dist<1e-6: #threshold for when points are so close that they are considered indistinguishable\n",
    "        expected_improvement=0.0\n",
    "        return expected_improvement\n",
    "\n",
    "    mu, sigma = gpr_obj.predict(x.reshape(1, -1), return_std=True)\n",
    "    mu_sample = gpr_obj.predict(Xsamples)\n",
    "    mu_sample_opt = np.min(Ysamples)\n",
    "\n",
    "    # In case sigma equals zero\n",
    "    with np.errstate(divide='ignore'):\n",
    "        Z = (mu_sample_opt-mu) / sigma\n",
    "        expected_improvement = (mu_sample_opt-mu) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        expected_improvement[sigma == 0.0] == 0.0\n",
    "    answer=-1.*expected_improvement #to maximize EI, you minimize the negative of it\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d30aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_the_ei(gpr_obj, X, Y):\n",
    "    x = np.atleast_2d(np.linspace(0, 10, 1000)).T \n",
    "    expimp=np.zeros(1000)\n",
    "    for ii in range(1000):\n",
    "        expimp[ii] = -ei(x[ii], gpr_obj, X, Y)\n",
    "    plt.figure()\n",
    "    plt.plot(x, expimp, 'k--', label='Expected improvement')\n",
    "    plt.plot(X, np.zeros(X.shape[0]), 'rx', markersize=10, label='Observation sites')\n",
    "    #plt.plot(X[-1],0, 'gs', markersize=10, label='Newest sample')\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$EI(x)$')\n",
    "    #plt.ylim(-10, 20)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc130312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do your GP iterations: maximize EI, select new point, evaluate new point, update GP, maximize EI, ....\n",
    "n_GP_samples = 20 # allow 50 evaluations of f\n",
    "bound_list = np.array([[xlow, xup]])\n",
    "xnew=[]\n",
    "while  X.shape[0]< n_GP_samples: \n",
    "    gpr_obj = GaussianProcessRegressor(kernel=kernel, random_state=0,normalize_y=True, n_restarts_optimizer=10).fit(X, y) #create the GP\n",
    "    plot_the_gp(X, y, gpr_obj, xnew)\n",
    "    plot_the_ei(gpr_obj, X, y)\n",
    "    #compute next point by maximizing expected improvement, multi-start optimization\n",
    "    xnew = []\n",
    "    fnew =np.inf\n",
    "    for ii in range(10):\n",
    "        x0 = xlow + (xup-xlow) * np.random.rand(1,dim) #random starting point for optimizing expected improvement\n",
    "        res= minimize(ei,np.ravel(x0),method='SLSQP',bounds=bound_list, args=(gpr_obj, X, y))\t\n",
    "        dist = np.min(scp.distance.cdist(np.asmatrix(res.x), X)) #make sure new point is sufficiently far away from already sampled points\n",
    "        if np.min(dist)>1e-6 and res.success: #1e-3 is tunable\n",
    "            x_ = np.asmatrix(res.x)\n",
    "            if res.fun< fnew:\n",
    "                xnew = x_\n",
    "                fnew = res.fun\n",
    "        else: #use random point as new point\n",
    "            x_ = np.asarray(xlow) + np.asarray(xup-xlow) * np.asarray(np.random.rand(1,dim)) #random starting point\n",
    "            fv= ei(x_, gpr_obj, X, y)\n",
    "            if len(xnew)== 0 or fv < fnew:\n",
    "                xnew = np.asmatrix(x_)\n",
    "                fnew= fv\n",
    "\n",
    "                \n",
    "    fval = f(np.ravel(xnew))\n",
    "\n",
    "    #update Xsamples and Ysamples arrays\n",
    "    X=np.concatenate((X, np.asmatrix(xnew)), axis = 0)\n",
    "    Y_ = np.zeros(len(y)+1)    \n",
    "    Y_[0:len(y)]= y\n",
    "    Y_[-1]=fval\n",
    "    y =Y_\n",
    "    minID=np.argmin(y) #find index of best point\n",
    "    print('best point: ', X[minID])\n",
    "    print('best value: ', y[minID])\n",
    "    print('Number evaluations: ', X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6629a24",
   "metadata": {},
   "source": [
    "From the images of the expected improvement we can see that the peaks are becoming increasingly narrow, to almost looking like jump-discontinuities. This means that for an optimizer that tries to find the maximum of the expected improvement function, it becomes increasingly harder to find the optimum and sampling becomes more \"random\" in the space because the function is flat and EI values are the same everywhere except at the jumps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcefb24",
   "metadata": {},
   "source": [
    "Takeaways: \n",
    "- GPs can be useful to guide the search during optimization\n",
    "- They shine when the number of function evaluations is severely limited\n",
    "- The expected improvement function helps to select points that are the \"most promising\" next evaluations\n",
    "- The expected improvement function is multimodal and becomes increasingly harder to optimize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
